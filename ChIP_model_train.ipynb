{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import importlib\n",
    "\n",
    "import models\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import utils\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from '/home/alex/SCerevisiae_chromatin_NN_prediction/models.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 101, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = 101\n",
    "model = models.SiameseInceptionNetwork(length)\n",
    "x1 = torch.tensor(utils.idx_to_onehot(np.random.randint(4, size=(2, length))))\n",
    "x2 = torch.tensor(utils.idx_to_onehot(np.random.randint(4, size=(2, length))))\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4993],\n",
       "        [-0.1610]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(x1, x2)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6256, dtype=torch.float64,\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.tensor([[1], [1]], dtype=float).to(pred.device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(weight=weight)\n",
    "loss = loss_fn(pred, torch.tensor([[0], [1]], dtype=float).to(pred.device))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fastq(filename):\n",
    "    if filename.endswith(\"gz\"):\n",
    "        with gzip.open(filename, \"rt\") as f:\n",
    "            for read in SeqIO.parse(f, format=\"fastq\"):\n",
    "                yield read\n",
    "    else:\n",
    "        for read in SeqIO.parse(filename, \"fastq\"):\n",
    "            yield read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in [\n",
    "    f\"/home/alex/shared_folder/ChIP_ENCODE/random{i}.fastq.gz\" for i in range(1, 3)\n",
    "]:\n",
    "    with gzip.open(filename, \"wt\") as f:\n",
    "        phred = \"J\" * 101\n",
    "        seqs = [\n",
    "            \"\".join(seq)\n",
    "            for seq in np.array(list(\"ACGT\"))[\n",
    "                np.random.randint(4, size=(100000, len(phred)))\n",
    "            ]\n",
    "        ]\n",
    "        for i in range(len(seqs)):\n",
    "            f.write(f\"@read_{i}\\n{seqs[i]}\\n+\\n{phred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2, 101)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 0, 3, 0, 2, 2, 1, 3, 0, 3, 2, 3, 1, 3, 3, 2, 2, 3, 1, 2,\n",
       "        2, 3, 2, 2, 1, 1, 1, 0, 3, 1, 3, 3, 1, 3, 3, 2, 0, 1, 2, 2, 3, 2,\n",
       "        1, 2, 2, 1, 0, 2, 1, 1, 3, 3, 3, 2, 2, 1, 1, 0, 0, 2, 3, 0, 2, 3,\n",
       "        3, 1, 0, 0, 2, 0, 1, 3, 0, 0, 3, 3, 0, 1, 0, 3, 3, 2, 3, 3, 2, 3,\n",
       "        1, 3, 0, 3, 2, 1, 1, 2, 0, 3, 0, 3, 0],\n",
       "       [1, 2, 0, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 0, 1, 3, 1, 2, 3, 0, 1, 1,\n",
       "        2, 1, 2, 0, 3, 1, 2, 3, 1, 3, 3, 1, 1, 3, 1, 2, 3, 3, 2, 0, 0, 0,\n",
       "        0, 2, 0, 0, 1, 2, 3, 1, 3, 3, 2, 3, 2, 3, 0, 3, 1, 0, 3, 2, 3, 0,\n",
       "        1, 1, 1, 3, 2, 1, 3, 0, 3, 2, 2, 0, 3, 3, 2, 1, 2, 2, 1, 2, 3, 1,\n",
       "        3, 1, 0, 3, 2, 0, 1, 2, 0, 1, 3, 3, 2]], dtype=int8)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids, reads = [], []\n",
    "for read1, read2 in zip(\n",
    "    parse_fastq(\"/home/alex/shared_folder/ChIP_ENCODE/random1.fastq.gz\"),\n",
    "    parse_fastq(\"/home/alex/shared_folder/ChIP_ENCODE/random2.fastq.gz\"),\n",
    "):\n",
    "    if read1.id != read2.id:\n",
    "        raise ValueError(\"Files do not have same ids\")\n",
    "    ids.append(read1.id)\n",
    "    reads.extend([read1, read2])\n",
    "maxlen = max(len(read) for read in reads)\n",
    "minlen = min(len(read) for read in reads)\n",
    "if minlen != maxlen:\n",
    "    reads = [read + \"N\" * (maxlen - len(read)) for read in reads]\n",
    "reads = np.stack(utils.ordinal_encoder(reads)).reshape(-1, 2, maxlen)\n",
    "print(reads.shape)\n",
    "reads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2, 101)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 0, 3, 0, 2, 2, 1, 3, 0, 3, 2, 3, 1, 3, 3, 2, 2, 3, 1, 2,\n",
       "        2, 3, 2, 2, 1, 1, 1, 0, 3, 1, 3, 3, 1, 3, 3, 2, 0, 1, 2, 2, 3, 2,\n",
       "        1, 2, 2, 1, 0, 2, 1, 1, 3, 3, 3, 2, 2, 1, 1, 0, 0, 2, 3, 0, 2, 3,\n",
       "        3, 1, 0, 0, 2, 0, 1, 3, 0, 0, 3, 3, 0, 1, 0, 3, 3, 2, 3, 3, 2, 3,\n",
       "        1, 3, 0, 3, 2, 1, 1, 2, 0, 3, 0, 3, 0],\n",
       "       [1, 2, 0, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 0, 1, 3, 1, 2, 3, 0, 1, 1,\n",
       "        2, 1, 2, 0, 3, 1, 2, 3, 1, 3, 3, 1, 1, 3, 1, 2, 3, 3, 2, 0, 0, 0,\n",
       "        0, 2, 0, 0, 1, 2, 3, 1, 3, 3, 2, 3, 2, 3, 0, 3, 1, 0, 3, 2, 3, 0,\n",
       "        1, 1, 1, 3, 2, 1, 3, 0, 3, 2, 2, 0, 3, 3, 2, 1, 2, 2, 1, 2, 3, 1,\n",
       "        3, 1, 0, 3, 2, 0, 1, 2, 0, 1, 3, 3, 2]], dtype=int8)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids, reads = [], []\n",
    "for read1, read2 in zip(\n",
    "    *(\n",
    "        parse_fastq(filename)\n",
    "        for filename in (\n",
    "            \"/home/alex/shared_folder/ChIP_ENCODE/random1.fastq.gz\",\n",
    "            \"/home/alex/shared_folder/ChIP_ENCODE/random2.fastq.gz\",\n",
    "        )\n",
    "    )\n",
    "):\n",
    "    if read1.id != read2.id:\n",
    "        raise ValueError(\"Files do not have same ids\")\n",
    "    ids.append(read1.id)\n",
    "    reads.extend([read1, read2])\n",
    "maxlen = max(len(read) for read in reads)\n",
    "minlen = min(len(read) for read in reads)\n",
    "if minlen != maxlen:\n",
    "    reads = [read + \"N\" * (maxlen - len(read)) for read in reads]\n",
    "reads = np.stack(utils.ordinal_encoder(reads)).reshape(-1, 2, maxlen)\n",
    "print(reads.shape)\n",
    "reads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{101: 100000}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lendict = {}\n",
    "for read in reads:\n",
    "    if len(read) in lendict:\n",
    "        lendict[len(read)] += 1\n",
    "    else:\n",
    "        lendict[len(read)] = 1\n",
    "lendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataRAM = train_pytorch.SequenceDatasetRAM(\n",
    "    args.fasta_file,\n",
    "    args.label_files,\n",
    "    [f\"chr{i}\" for i in [\"I\", \"II\"]],\n",
    "    winsize=2048,\n",
    "    head_interval=args.head_interval,\n",
    "    head_crop=args.head_crop,\n",
    "    strand=args.strand,\n",
    "    remove0s=args.remove0s,\n",
    "    removeNs=args.removeNs,\n",
    "    transform=utils.idx_to_onehot,\n",
    ")\n",
    "train_dataloaderRAM = DataLoader(\n",
    "    training_dataRAM,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import pyBigWig as pbw\n",
    "from scipy.signal import convolve\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import train_DeepChIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_indices_by_file(self, indices):\n",
    "#     \"\"\"Group indices by files they fall into, and change to indices from file start.\"\"\"\n",
    "#     file_idx, idx_mod = divmod(indices, self.shard_size)\n",
    "#     return {i: idx_mod[file_idx == i] for i in range(4)}\n",
    "\n",
    "# def read_with_mmap(self, indices):\n",
    "#     \"\"\"Get reads associated to indices.\"\"\"\n",
    "#     # Group indices by files\n",
    "#     indices_by_file = self.get_indices_by_file(indices)\n",
    "#     reads = []\n",
    "#     for i, length in enumerate(self.file_lengths):\n",
    "#         # Use memmap to load specific indices\n",
    "#         reads.append(\n",
    "#             np.memmap(\n",
    "#                 Path(self.dirname, f\"{self.split}_{i}.npy\"),\n",
    "#                 dtype=self.dtype,\n",
    "#                 shape=(length,) + self.read_shape,\n",
    "#                 mode=\"r\",\n",
    "#                 offset=128,\n",
    "#             )[indices_by_file[i]]\n",
    "#         )\n",
    "#     return np.concatenate(reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"paired_sharded_dataset\"\n",
    "num_workers = 1\n",
    "paired = True\n",
    "balance_classes = False\n",
    "batch_size = 1024\n",
    "training_data = train_DeepChIP.DatasetFromFiles(\n",
    "    dataset_dir,\n",
    "    split='train',\n",
    "    strand='for',\n",
    "    transform=utils.idx_to_onehot,\n",
    ")\n",
    "valid_data = train_DeepChIP.DatasetFromFiles(\n",
    "    dataset_dir,\n",
    "    split='valid',\n",
    "    strand='for',\n",
    "    transform=utils.idx_to_onehot,\n",
    "    paired=paired\n",
    ")\n",
    "if balance_classes:\n",
    "    def collate_fn(batch):\n",
    "        return train_DeepChIP.collate_classbalance(batch)\n",
    "else:\n",
    "    collate_fn = None\n",
    "train_dataloader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1024, 101, 4])\n",
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         ...,\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.]]])\n",
      "3.8413918018341064\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for i, (X, y, w) in enumerate(train_dataloader):\n",
    "    print(len(X))\n",
    "    print(X[0].shape)\n",
    "    print(X[0])\n",
    "    print(time.time() - t0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61308940"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = train_DeepChIP.DatasetFromFiles(\"paired_sharded_dataset\", \"train\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 4) (101, 4) 1 1\n"
     ]
    }
   ],
   "source": [
    "(x1, x2), y, w = dataset[0]\n",
    "print(x1.shape, x2.shape, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1876179/4080736814.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1061.4660887842454, 2052.456451496042)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "label = np.random.randint(2, size=batch_size) != 0\n",
    "weight = 1.5 + np.random.random(size=batch_size)\n",
    "\n",
    "new_weight.sum(), weight.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_byclass1(label, weight):\n",
    "    dico = {}\n",
    "    for lab, w in zip(label, weight):\n",
    "        if lab in dico: dico[lab] += w\n",
    "        else: dico[lab] = w\n",
    "    return dico\n",
    "\n",
    "def sum_byclass2(label, weight):\n",
    "    return {lab: np.sum(weight[label == lab]) for lab in np.unique(label)}\n",
    "\n",
    "def sum_byclass3(label, weight):\n",
    "    df = pd.DataFrame({\"label\": label, \"weight\": weight})\n",
    "    return df.groupby(\"label\").sum()\n",
    "\n",
    "def sum_byclass4(label, weight):\n",
    "    y = label\n",
    "    tot = np.size(weight)\n",
    "    tot_pos = np.sum(np.where(y, weight, 0))\n",
    "    tot_neg = np.sum(np.where(y, weight, 0))\n",
    "    return np.where(\n",
    "        y,\n",
    "        weight*tot / (2*tot_pos),\n",
    "        weight*tot / (2*tot_neg)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 µs ± 195 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "82.5 µs ± 535 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "515 µs ± 490 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "label = np.random.randint(10, size=batch_size)\n",
    "weight = .5 + np.random.random(size=batch_size)\n",
    "%timeit sum_byclass1(label, weight)\n",
    "%timeit sum_byclass2(label, weight)\n",
    "%timeit sum_byclass3(label, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 µs ± 3.25 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "25 µs ± 101 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "501 µs ± 999 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "18.9 µs ± 318 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "label = np.random.randint(2, size=batch_size) != 0\n",
    "weight = .5 + np.random.random(size=batch_size)\n",
    "%timeit sum_byclass1(label, weight)\n",
    "%timeit sum_byclass2(label, weight)\n",
    "%timeit sum_byclass3(label, weight)\n",
    "%timeit sum_byclass4(label, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24 ms ± 1.21 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "181 µs ± 575 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "605 µs ± 897 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024*10\n",
    "label = np.random.randint(2, size=batch_size) != 0\n",
    "weight = .5 + np.random.random(size=batch_size)\n",
    "%timeit sum_byclass1(label, weight)\n",
    "%timeit sum_byclass2(label, weight)\n",
    "%timeit sum_byclass3(label, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5 ms ± 226 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.73 ms ± 4.05 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "1.42 ms ± 2.75 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024*100\n",
    "label = np.random.randint(2, size=batch_size) != 0\n",
    "weight = .5 + np.random.random(size=batch_size)\n",
    "%timeit sum_byclass1(label, weight)\n",
    "%timeit sum_byclass2(label, weight)\n",
    "%timeit sum_byclass3(label, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0890798568725586\n",
      "100 0.544684648513794\n",
      "200 0.9197783470153809\n",
      "300 1.2938051223754883\n",
      "400 1.6689815521240234\n",
      "500 2.1042237281799316\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for i, ((x1, x2), y, w) in enumerate(dataloader):\n",
    "    if i % 100 == 0:\n",
    "        print(i, time.time() - t0)\n",
    "    if i > 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices_by_file(indices, shard_size):\n",
    "    file_idx, idx_mod = divmod(indices, shard_size)\n",
    "    return {i: idx_mod[file_idx == i] for i in range(4)}\n",
    "\n",
    "\n",
    "def read_with_mmap(indices):\n",
    "    split = \"train\"\n",
    "    dataset = \"paired_sharded_dataset\"\n",
    "    n_files = 4\n",
    "    shard_size = 2**24\n",
    "    last_shard_size = 10_977_292\n",
    "    shape = (shard_size, 2, 101)\n",
    "    dtype = np.int8\n",
    "    indices_by_file = get_indices_by_file(indices, shard_size)\n",
    "    reads = []\n",
    "    for i, shard_size in enumerate([shard_size] * (n_files - 1) + [last_shard_size]):\n",
    "        reads.append(\n",
    "            np.memmap(\n",
    "                Path(dataset, f\"{split}_{i}.npy\"),\n",
    "                dtype=dtype,\n",
    "                shape=(shard_size,) + shape[1:],\n",
    "                mode=\"r\",\n",
    "                offset=128,\n",
    "            )[indices_by_file[i]]\n",
    "        )\n",
    "    return np.concatenate(reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing paired_sharded_dataset/train_3.npy\n",
      "writing paired_sharded_dataset/train_0.npy\n",
      "writing paired_sharded_dataset/train_2.npy\n",
      "writing paired_sharded_dataset/valid_0.npy\n",
      "writing paired_sharded_dataset/train_1.npy\n"
     ]
    }
   ],
   "source": [
    "for file in Path(\"paired_sharded_dataset\").glob(\"*.npz\"):\n",
    "    npyfile = Path(file.parent, file.stem + \".npy\")\n",
    "    if not npyfile.exists():\n",
    "        print(f\"writing {npyfile}\")\n",
    "        with np.load(file) as f:\n",
    "            np.save(npyfile, f[\"reads\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 ms ± 618 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "506 ms ± 824 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "19.7 µs ± 368 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "19.5 µs ± 398 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit reads = np.load(\"paired_sharded_dataset/test_0.npy\")\n",
    "%timeit reads = np.load(\"paired_sharded_dataset/train_0.npy\")\n",
    "%timeit reads = np.memmap(\"paired_sharded_dataset/test_0.npy\", dtype='int8', shape=(2**23, 2, 101), mode='r', offset=128)\n",
    "%timeit reads = np.memmap(\"paired_sharded_dataset/train_0.npy\", dtype='int8', shape=(2**24, 2, 101), mode='r', offset=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50331648, 2, 101)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_shards = []\n",
    "for i in range(3):\n",
    "    train_shards.append(np.load(f\"paired_sharded_dataset/train_{i}.npy\"))\n",
    "train_shard = np.concatenate(train_shards, axis=0)\n",
    "train_shard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1412229,  7234354, 11941675, ..., 15862277,  1245288, 10982186])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.random.randint(2**24, size=1024)\n",
    "while len(np.unique(indices)) != len(indices):\n",
    "    indices = np.random.randint(2**24, size=1024)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 ms ± 599 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "533 ms ± 404 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.84 ms ± 8.69 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "2.09 ms ± 5.03 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit reads = np.load(\"paired_sharded_dataset/test_0.npy\")[indices]\n",
    "%timeit reads = np.load(\"paired_sharded_dataset/train_0.npy\")[indices]\n",
    "%timeit reads = np.memmap(\"paired_sharded_dataset/test_0.npy\", dtype='int8', shape=(2**23, 2, 101), mode='r', offset=128)[indices]\n",
    "%timeit reads = np.memmap(\"paired_sharded_dataset/train_0.npy\", dtype='int8', shape=(2**24, 2, 101), mode='r', offset=128)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_mmap():\n",
    "    reads = []\n",
    "    for idx in indices:\n",
    "        reads.append(\n",
    "            np.memmap(\n",
    "                \"paired_sharded_dataset/train_0.npy\",\n",
    "                dtype=\"int8\",\n",
    "                shape=(2**24, 2, 101),\n",
    "                mode=\"r\",\n",
    "                offset=128,\n",
    "            )[idx]\n",
    "        )\n",
    "    return np.concatenate(reads)\n",
    "\n",
    "\n",
    "def single_mmap():\n",
    "    return np.memmap(\n",
    "        \"paired_sharded_dataset/train_0.npy\",\n",
    "        dtype=\"int8\",\n",
    "        shape=(2**24, 2, 101),\n",
    "        mode=\"r\",\n",
    "        offset=128,\n",
    "    )[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.3 ms ± 67.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "2.18 ms ± 13.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit loop_mmap()\n",
    "%timeit single_mmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_by_file = get_indices_by_file(indices, 2**24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.random(1000).astype(\"float32\")\n",
    "np.save(\"float32.npy\", arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39248276, 0.40912217, 0.44340226, 0.80827844, 0.31452698,\n",
       "       0.53480494, 0.2300319 , 0.21574275, 0.6069011 , 0.43160114],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([0.39248276, 0.40912217, 0.44340226, 0.80827844, 0.31452698,\n",
       "        0.53480494, 0.2300319 , 0.21574275, 0.6069011 , 0.43160114],\n",
       "       dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.memmap(\"float32.npy\", dtype=\"float32\", mode=\"r\", offset=128)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 2, 101), (1024, 2, 101))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reads = read_with_load()\n",
    "reads_mmap = read_with_memmap(indices)\n",
    "reads.shape, reads_mmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_mmap = read_with_memmap(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(reads == reads_mmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.89 s ± 792 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "2.4 ms ± 4.82 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit read_with_load()\n",
    "%timeit read_with_memmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = np.load(\"paired_sharded_dataset/train_0.npy\")\n",
    "train3 = np.load(\"paired_sharded_dataset/train_3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int8'), (10977292, 2, 101))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3.dtype, train3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmap_train0 = np.memmap(\n",
    "    \"paired_sharded_dataset/train_0.npy\",\n",
    "    dtype=\"int8\",\n",
    "    shape=(2**24, 2, 101),\n",
    "    mode=\"r\",\n",
    "    offset=128,\n",
    ")\n",
    "mmap_train1 = np.memmap(\n",
    "    \"paired_sharded_dataset/train_1.npy\",\n",
    "    dtype=\"int8\",\n",
    "    shape=(2**24, 2, 101),\n",
    "    mode=\"r\",\n",
    "    offset=128,\n",
    ")\n",
    "mmap_train2 = np.memmap(\n",
    "    \"paired_sharded_dataset/train_2.npy\",\n",
    "    dtype=\"int8\",\n",
    "    shape=(2**24, 2, 101),\n",
    "    mode=\"r\",\n",
    "    offset=128,\n",
    ")\n",
    "mmap_train3 = np.memmap(\n",
    "    \"paired_sharded_dataset/train_3.npy\",\n",
    "    dtype=\"int8\",\n",
    "    shape=(10_977_292, 2, 101),\n",
    "    mode=\"r\",\n",
    "    offset=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int8'), (10977292, 2, 101))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmap_train3.dtype, mmap_train3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(mmap_train3 == train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain3\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train3' is not defined"
     ]
    }
   ],
   "source": [
    "print(train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2 1 1 ... 0 3 2]\n",
      "  [1 1 1 ... 1 0 3]]\n",
      "\n",
      " [[1 3 2 ... 1 3 2]\n",
      "  [0 0 2 ... 1 1 0]]\n",
      "\n",
      " [[0 3 3 ... 1 0 1]\n",
      "  [3 2 2 ... 0 0 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2 2 0 ... 2 3 2]\n",
      "  [2 3 0 ... 0 1 1]]\n",
      "\n",
      " [[1 0 3 ... 1 0 2]\n",
      "  [1 0 0 ... 0 1 3]]\n",
      "\n",
      " [[0 2 2 ... 2 0 1]\n",
      "  [1 3 2 ... 1 1 3]]]\n"
     ]
    }
   ],
   "source": [
    "print(mmap_train3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flamingo-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
